{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9711346,"sourceType":"datasetVersion","datasetId":5940229}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 1) eliminar protobuf problemático\n!pip uninstall -y protobuf\n\n# 2) instalar una versión compatible (3.20.x suele resolverlo) y mediapipe\n!pip install protobuf==3.20.3 mediapipe -q\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T21:13:34.059956Z","iopub.execute_input":"2026-01-04T21:13:34.060164Z","iopub.status.idle":"2026-01-04T21:13:44.638400Z","shell.execute_reply.started":"2026-01-04T21:13:34.060147Z","shell.execute_reply":"2026-01-04T21:13:44.637429Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: protobuf 6.33.0\nUninstalling protobuf-6.33.0:\n  Successfully uninstalled protobuf-6.33.0\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m85.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.8/135.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nopentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 3.20.3 which is incompatible.\nonnx 1.18.0 requires protobuf>=4.25.1, but you have protobuf 3.20.3 which is incompatible.\na2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 3.20.3 which is incompatible.\nray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\ntensorflow-metadata 1.17.2 requires protobuf>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\ngrpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import cv2\nimport mediapipe as mp\nimport numpy as np\nimport os\nimport glob\nimport urllib.request\nimport time\nimport tracemalloc\nimport tempfile\nimport joblib\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom mediapipe.tasks import python\nfrom mediapipe.tasks.python import vision\nfrom scipy.interpolate import interp1d\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support, confusion_matrix\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nimport xgboost as xgb\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport random\n\n# Set Global Seeds for Reproducibility\nSEED = 42\nos.environ['PYTHONHASHSEED'] = str(SEED)\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)\n\n# Configure Plotting Style\nsns.set_style(\"whitegrid\")\nplt.rcParams.update({'font.size': 12})\n# Use Agg backend to save plots without display\nplt.switch_backend('Agg')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T21:13:44.639970Z","iopub.execute_input":"2026-01-04T21:13:44.640207Z","iopub.status.idle":"2026-01-04T21:14:10.116663Z","shell.execute_reply.started":"2026-01-04T21:13:44.640184Z","shell.execute_reply":"2026-01-04T21:14:10.116046Z"}},"outputs":[{"name":"stderr","text":"2026-01-04 21:13:48.610537: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1767561229.069506      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1767561229.199987      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"### 1. MediaPipe Setup (Updated for MediaPipe Tasks API)\n\ndef download_model(url, filename):\n    if not os.path.exists(filename):\n        print(f\"Downloading {filename}...\")\n        urllib.request.urlretrieve(url, filename)\n        print(\"Done.\")\n\ndownload_model('https://storage.googleapis.com/mediapipe-models/pose_landmarker/pose_landmarker_full/float16/1/pose_landmarker_full.task', 'pose_landmarker_full.task')\ndownload_model('https://storage.googleapis.com/mediapipe-models/hand_landmarker/hand_landmarker/float16/1/hand_landmarker.task', 'hand_landmarker.task')\n\nBaseOptions = mp.tasks.BaseOptions\nPoseLandmarker = mp.tasks.vision.PoseLandmarker\nPoseLandmarkerOptions = mp.tasks.vision.PoseLandmarkerOptions\nHandLandmarker = mp.tasks.vision.HandLandmarker\nHandLandmarkerOptions = mp.tasks.vision.HandLandmarkerOptions\nVisionRunningMode = mp.tasks.vision.RunningMode\n\npose_options = PoseLandmarkerOptions(\n    base_options=BaseOptions(model_asset_path='pose_landmarker_full.task'),\n    running_mode=VisionRunningMode.IMAGE,\n    min_pose_detection_confidence=0.5,\n    min_pose_presence_confidence=0.5,\n    min_tracking_confidence=0.5,\n    output_segmentation_masks=False\n)\npose_landmarker = PoseLandmarker.create_from_options(pose_options)\n\nhand_options = HandLandmarkerOptions(\n    base_options=BaseOptions(model_asset_path='hand_landmarker.task'),\n    running_mode=VisionRunningMode.IMAGE,\n    num_hands=2,\n    min_hand_detection_confidence=0.5,\n    min_hand_presence_confidence=0.5,\n    min_tracking_confidence=0.5\n)\nhand_landmarker = HandLandmarker.create_from_options(hand_options)\n\nPOSE_INDICES = [11, 12, 13, 14, 15, 16]\n\ndef get_landmarks(image):\n    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image)\n    \n    results_pose = pose_landmarker.detect(mp_image)\n    results_hands = hand_landmarker.detect(mp_image)\n    \n    if not results_pose.pose_landmarks:\n        return None\n        \n    pl = results_pose.pose_landmarks[0]\n\n    required_indices = [11, 12, 13, 14]\n    for idx in required_indices:\n        if pl[idx].visibility < 0.25:\n            return None\n    \n    left_shoulder = np.array([pl[11].x, pl[11].y])\n    right_shoulder = np.array([pl[12].x, pl[12].y])\n    \n    shoulder_center = (left_shoulder + right_shoulder) / 2.0\n    shoulder_width = np.linalg.norm(left_shoulder - right_shoulder)\n    \n    if shoulder_width < 1e-6: \n        shoulder_width = 1.0\n    \n    current_frame_points = []\n    \n    safe_wrists = {}\n    \n    for idx in POSE_INDICES:\n        target_landmark = pl[idx]\n        p = np.array([target_landmark.x, target_landmark.y])\n        \n        if idx == 15: # Muñeca Izq\n            if target_landmark.visibility < 0.25:\n                elbow = pl[13]\n                p = np.array([elbow.x, elbow.y])\n            safe_wrists['Left'] = p\n        \n        elif idx == 16: # Muñeca Der\n            if target_landmark.visibility < 0.25:\n                elbow = pl[14]\n                p = np.array([elbow.x, elbow.y])\n            safe_wrists['Right'] = p\n\n        norm_p = (p - shoulder_center) / shoulder_width\n        current_frame_points.extend(norm_p)\n         \n    left_hand_raw = None\n    right_hand_raw = None\n    \n    if results_hands.hand_landmarks:\n        for idx, hand_lm_list in enumerate(results_hands.hand_landmarks):\n            if idx < len(results_hands.handedness):\n                label = results_hands.handedness[idx][0].category_name\n            else:\n                label = 'Unknown'\n            \n            h_points = []\n            for hl in hand_lm_list:\n                h_points.append([hl.x, hl.y])\n            \n            if label == 'Left':\n                left_hand_raw = np.array(h_points)\n            else:\n                right_hand_raw = np.array(h_points)\n                \n    # Rellenar manos faltantes con posición de la muñeca\n    if left_hand_raw is None:\n        ref_point = safe_wrists.get('Left', np.zeros(2))\n        left_hand_raw = np.tile(ref_point, (21, 1))\n        \n    if right_hand_raw is None:\n        ref_point = safe_wrists.get('Right', np.zeros(2))\n        right_hand_raw = np.tile(ref_point, (21, 1))\n\n    # Normalizar manos\n    for point in left_hand_raw:\n        norm_p = (point - shoulder_center) / shoulder_width\n        current_frame_points.extend(norm_p)\n        \n    for point in right_hand_raw:\n        norm_p = (point - shoulder_center) / shoulder_width\n        current_frame_points.extend(norm_p)\n    \n    return current_frame_points\n\ndef interpolate_sequence(sequence, target_len=40):\n    sequence = np.array(sequence)\n    seq_len = len(sequence)\n    \n    if seq_len == 0:\n        return np.zeros((target_len, 96))\n    \n    if seq_len == 1:\n        return np.repeat(sequence, target_len, axis=0)\n        \n    current_x = np.linspace(0, 1, seq_len)\n    target_x = np.linspace(0, 1, target_len)\n    \n    f = interp1d(current_x, sequence, axis=0, kind='linear', fill_value=\"extrapolate\")\n    interpolated_sequence = f(target_x)\n    \n    return interpolated_sequence","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T21:14:10.117351Z","iopub.execute_input":"2026-01-04T21:14:10.117748Z","iopub.status.idle":"2026-01-04T21:14:10.640475Z","shell.execute_reply.started":"2026-01-04T21:14:10.117728Z","shell.execute_reply":"2026-01-04T21:14:10.639597Z"}},"outputs":[{"name":"stdout","text":"Downloading pose_landmarker_full.task...\nDone.\nDownloading hand_landmarker.task...\nDone.\n","output_type":"stream"},{"name":"stderr","text":"INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nW0000 00:00:1767561250.523626     128 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\nW0000 00:00:1767561250.575218     128 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\nW0000 00:00:1767561250.615007     132 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\nW0000 00:00:1767561250.636649     132 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"### 3. Data Loading\nDATA_PATH = \"/kaggle/input/lsa-64-dinamico/all/*.mp4\"\nfiles = glob.glob(DATA_PATH)\nprint(f\"Found {len(files)} files.\")\n\nX_seq_list = [] # Para almacenar secuencias (N, TimeSteps, Features)\ny_list = []\nskipped_count = 0\n\nfor i, file_path in enumerate(files):\n    filename = os.path.basename(file_path)\n    label = filename.split('_')[0]\n    \n    # Filtrar solo tus clases deseadas si es necesario, o usar todas\n    # allowed_labels = [\"001\", \"002\"] # Ejemplo\n    # if label not in allowed_labels: continue\n\n    if i % 10 == 0: print(f\"Processing {i}/{len(files)}: {filename}\")\n    \n    cap = cv2.VideoCapture(file_path)\n    video_landmarks = []\n    \n    while cap.isOpened():\n        ret, frame = cap.read()\n        if not ret:\n            break\n            \n        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        points = get_landmarks(image)\n        if points is not None:\n             video_landmarks.append(points)\n             \n    cap.release()\n    \n    if len(video_landmarks) > 0:\n        video_landmarks_interp = interpolate_sequence(video_landmarks, target_len=40)\n        X_seq_list.append(video_landmarks_interp) # Shape: (40, 96)\n        y_list.append(label)\n    else:\n        print(f\"⚠️ VIDEO DESCARTADO: {filename}\")\n        skipped_count += 1\n\nX_seq = np.array(X_seq_list) # Data secuencial: (N, 40, 96)\ny = np.array(y_list)\n\nif len(X_seq) == 0:\n    print(\"No data loaded. Change DATA_PATH or check files.\")\n    exit()\n\n# Flatten data for Traditional ML (N, 40*96) -> (N, 3840)\nX_flat = X_seq.reshape(X_seq.shape[0], -1)\n\n# Encode Labels\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(y)\nnum_classes = len(label_encoder.classes_)\nclass_names = label_encoder.classes_\n\nprint(f\"\\nProcesamiento finalizado.\")\nprint(f\"Total Videos: {len(X_seq)}\")\nprint(f\"Data Secuencial Shape (DL): {X_seq.shape}\")\nprint(f\"Data Plana Shape (ML): {X_flat.shape}\")\nprint(f\"Clases detectadas: {num_classes}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-04T21:14:10.641975Z","iopub.execute_input":"2026-01-04T21:14:10.642368Z","iopub.status.idle":"2026-01-05T06:23:08.394136Z","shell.execute_reply.started":"2026-01-04T21:14:10.642347Z","shell.execute_reply":"2026-01-05T06:23:08.393297Z"}},"outputs":[{"name":"stdout","text":"Found 3200 files.\nProcessing 0/3200: 006_006_005.mp4\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1767561250.987221     126 landmark_projection_calculator.cc:78] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n","output_type":"stream"},{"name":"stdout","text":"Processing 10/3200: 027_002_003.mp4\nProcessing 20/3200: 064_003_004.mp4\nProcessing 30/3200: 015_005_003.mp4\nProcessing 40/3200: 002_002_001.mp4\nProcessing 50/3200: 023_006_002.mp4\nProcessing 60/3200: 034_009_002.mp4\nProcessing 70/3200: 045_006_004.mp4\nProcessing 80/3200: 018_001_005.mp4\nProcessing 90/3200: 032_002_005.mp4\nProcessing 100/3200: 053_001_004.mp4\nProcessing 110/3200: 001_004_001.mp4\nProcessing 120/3200: 061_010_001.mp4\nProcessing 130/3200: 047_007_004.mp4\nProcessing 140/3200: 055_006_001.mp4\nProcessing 150/3200: 025_006_001.mp4\nProcessing 160/3200: 034_010_005.mp4\nProcessing 170/3200: 026_007_003.mp4\nProcessing 180/3200: 035_001_003.mp4\nProcessing 190/3200: 058_005_002.mp4\nProcessing 200/3200: 001_006_002.mp4\nProcessing 210/3200: 050_009_001.mp4\nProcessing 220/3200: 046_002_003.mp4\nProcessing 230/3200: 047_001_004.mp4\nProcessing 240/3200: 057_009_005.mp4\nProcessing 250/3200: 064_007_001.mp4\nProcessing 260/3200: 045_002_004.mp4\nProcessing 270/3200: 039_005_001.mp4\nProcessing 280/3200: 063_008_005.mp4\nProcessing 290/3200: 027_007_002.mp4\nProcessing 300/3200: 034_002_004.mp4\nProcessing 310/3200: 055_008_002.mp4\nProcessing 320/3200: 024_003_003.mp4\nProcessing 330/3200: 043_005_003.mp4\nProcessing 340/3200: 022_002_001.mp4\nProcessing 350/3200: 035_006_005.mp4\nProcessing 360/3200: 014_007_002.mp4\nProcessing 370/3200: 054_010_003.mp4\nProcessing 380/3200: 060_002_003.mp4\nProcessing 390/3200: 051_009_003.mp4\nProcessing 400/3200: 046_010_001.mp4\nProcessing 410/3200: 050_001_002.mp4\nProcessing 420/3200: 028_002_002.mp4\nProcessing 430/3200: 007_004_002.mp4\nProcessing 440/3200: 060_006_002.mp4\nProcessing 450/3200: 045_008_001.mp4\nProcessing 460/3200: 032_001_003.mp4\nProcessing 470/3200: 060_004_004.mp4\nProcessing 480/3200: 060_009_005.mp4\nProcessing 490/3200: 035_009_002.mp4\nProcessing 500/3200: 002_007_005.mp4\nProcessing 510/3200: 002_010_002.mp4\nProcessing 520/3200: 039_003_003.mp4\nProcessing 530/3200: 040_008_002.mp4\nProcessing 540/3200: 004_003_003.mp4\nProcessing 550/3200: 038_005_002.mp4\nProcessing 560/3200: 037_006_002.mp4\nProcessing 570/3200: 049_001_001.mp4\nProcessing 580/3200: 061_002_003.mp4\nProcessing 590/3200: 037_001_005.mp4\nProcessing 600/3200: 028_001_003.mp4\nProcessing 610/3200: 017_004_005.mp4\nProcessing 620/3200: 028_009_002.mp4\nProcessing 630/3200: 049_009_005.mp4\nProcessing 640/3200: 024_005_001.mp4\nProcessing 650/3200: 025_004_005.mp4\nProcessing 660/3200: 025_006_002.mp4\nProcessing 670/3200: 037_004_004.mp4\nProcessing 680/3200: 058_010_002.mp4\nProcessing 690/3200: 010_005_004.mp4\nProcessing 700/3200: 016_002_003.mp4\nProcessing 710/3200: 003_001_002.mp4\nProcessing 720/3200: 044_006_005.mp4\nProcessing 730/3200: 004_006_003.mp4\nProcessing 740/3200: 021_002_005.mp4\nProcessing 750/3200: 032_006_005.mp4\nProcessing 760/3200: 062_002_005.mp4\nProcessing 770/3200: 040_005_005.mp4\nProcessing 780/3200: 045_006_001.mp4\nProcessing 790/3200: 026_010_005.mp4\nProcessing 800/3200: 029_007_005.mp4\nProcessing 810/3200: 031_002_003.mp4\nProcessing 820/3200: 008_008_002.mp4\nProcessing 830/3200: 061_008_005.mp4\nProcessing 840/3200: 006_007_004.mp4\nProcessing 850/3200: 015_003_004.mp4\nProcessing 860/3200: 010_003_002.mp4\nProcessing 870/3200: 027_009_005.mp4\nProcessing 880/3200: 042_001_003.mp4\nProcessing 890/3200: 019_008_005.mp4\nProcessing 900/3200: 056_004_001.mp4\nProcessing 910/3200: 032_005_001.mp4\nProcessing 920/3200: 048_003_001.mp4\nProcessing 930/3200: 032_006_003.mp4\nProcessing 940/3200: 062_004_001.mp4\nProcessing 950/3200: 064_005_002.mp4\nProcessing 960/3200: 012_010_005.mp4\nProcessing 970/3200: 001_005_005.mp4\nProcessing 980/3200: 043_009_003.mp4\nProcessing 990/3200: 038_006_003.mp4\nProcessing 1000/3200: 031_001_005.mp4\nProcessing 1010/3200: 052_001_005.mp4\nProcessing 1020/3200: 027_002_002.mp4\nProcessing 1030/3200: 017_010_002.mp4\nProcessing 1040/3200: 020_001_002.mp4\nProcessing 1050/3200: 040_006_002.mp4\nProcessing 1060/3200: 024_002_002.mp4\nProcessing 1070/3200: 020_002_002.mp4\nProcessing 1080/3200: 014_008_004.mp4\nProcessing 1090/3200: 001_007_002.mp4\nProcessing 1100/3200: 040_008_001.mp4\nProcessing 1110/3200: 032_008_002.mp4\nProcessing 1120/3200: 059_010_001.mp4\nProcessing 1130/3200: 024_010_005.mp4\nProcessing 1140/3200: 045_002_005.mp4\nProcessing 1150/3200: 014_006_001.mp4\nProcessing 1160/3200: 056_009_005.mp4\nProcessing 1170/3200: 058_003_004.mp4\nProcessing 1180/3200: 012_005_001.mp4\nProcessing 1190/3200: 029_008_005.mp4\nProcessing 1200/3200: 047_002_004.mp4\nProcessing 1210/3200: 044_010_005.mp4\nProcessing 1220/3200: 043_008_003.mp4\nProcessing 1230/3200: 057_008_005.mp4\nProcessing 1240/3200: 004_005_003.mp4\nProcessing 1250/3200: 056_007_004.mp4\nProcessing 1260/3200: 031_005_003.mp4\nProcessing 1270/3200: 002_008_004.mp4\nProcessing 1280/3200: 023_004_002.mp4\nProcessing 1290/3200: 030_009_001.mp4\nProcessing 1300/3200: 049_007_001.mp4\nProcessing 1310/3200: 036_008_001.mp4\nProcessing 1320/3200: 059_001_002.mp4\nProcessing 1330/3200: 064_002_002.mp4\nProcessing 1340/3200: 056_007_005.mp4\nProcessing 1350/3200: 025_005_001.mp4\nProcessing 1360/3200: 051_007_004.mp4\nProcessing 1370/3200: 056_003_002.mp4\nProcessing 1380/3200: 057_010_003.mp4\nProcessing 1390/3200: 064_008_003.mp4\nProcessing 1400/3200: 009_002_004.mp4\nProcessing 1410/3200: 008_007_003.mp4\nProcessing 1420/3200: 046_003_005.mp4\nProcessing 1430/3200: 019_004_003.mp4\nProcessing 1440/3200: 026_001_001.mp4\nProcessing 1450/3200: 028_001_001.mp4\nProcessing 1460/3200: 001_003_004.mp4\nProcessing 1470/3200: 003_007_001.mp4\nProcessing 1480/3200: 034_001_003.mp4\nProcessing 1490/3200: 023_010_005.mp4\nProcessing 1500/3200: 046_006_004.mp4\nProcessing 1510/3200: 040_001_003.mp4\nProcessing 1520/3200: 010_005_005.mp4\nProcessing 1530/3200: 055_003_004.mp4\nProcessing 1540/3200: 028_002_004.mp4\nProcessing 1550/3200: 013_008_004.mp4\nProcessing 1560/3200: 048_010_003.mp4\nProcessing 1570/3200: 038_007_002.mp4\nProcessing 1580/3200: 043_004_002.mp4\nProcessing 1590/3200: 045_008_004.mp4\nProcessing 1600/3200: 012_006_003.mp4\nProcessing 1610/3200: 019_010_005.mp4\nProcessing 1620/3200: 048_005_002.mp4\nProcessing 1630/3200: 013_008_003.mp4\nProcessing 1640/3200: 028_003_002.mp4\nProcessing 1650/3200: 056_003_005.mp4\nProcessing 1660/3200: 013_006_002.mp4\nProcessing 1670/3200: 031_006_004.mp4\nProcessing 1680/3200: 052_006_002.mp4\nProcessing 1690/3200: 060_007_001.mp4\nProcessing 1700/3200: 038_009_002.mp4\nProcessing 1710/3200: 030_008_003.mp4\nProcessing 1720/3200: 036_002_001.mp4\nProcessing 1730/3200: 022_005_005.mp4\nProcessing 1740/3200: 008_006_002.mp4\nProcessing 1750/3200: 019_003_001.mp4\nProcessing 1760/3200: 055_006_003.mp4\nProcessing 1770/3200: 028_008_001.mp4\nProcessing 1780/3200: 050_003_002.mp4\nProcessing 1790/3200: 024_009_005.mp4\nProcessing 1800/3200: 021_007_005.mp4\nProcessing 1810/3200: 036_005_002.mp4\nProcessing 1820/3200: 009_009_003.mp4\nProcessing 1830/3200: 052_004_005.mp4\nProcessing 1840/3200: 046_004_005.mp4\nProcessing 1850/3200: 004_005_002.mp4\nProcessing 1860/3200: 007_002_002.mp4\nProcessing 1870/3200: 002_008_001.mp4\nProcessing 1880/3200: 032_008_004.mp4\nProcessing 1890/3200: 054_004_001.mp4\nProcessing 1900/3200: 038_008_001.mp4\nProcessing 1910/3200: 049_010_001.mp4\nProcessing 1920/3200: 037_009_004.mp4\nProcessing 1930/3200: 003_006_001.mp4\nProcessing 1940/3200: 012_004_002.mp4\nProcessing 1950/3200: 014_007_003.mp4\nProcessing 1960/3200: 002_002_004.mp4\nProcessing 1970/3200: 057_005_004.mp4\nProcessing 1980/3200: 029_009_002.mp4\nProcessing 1990/3200: 054_002_003.mp4\nProcessing 2000/3200: 049_003_001.mp4\nProcessing 2010/3200: 017_001_004.mp4\nProcessing 2020/3200: 017_009_001.mp4\nProcessing 2030/3200: 063_010_005.mp4\nProcessing 2040/3200: 026_003_003.mp4\nProcessing 2050/3200: 006_007_001.mp4\nProcessing 2060/3200: 038_007_005.mp4\nProcessing 2070/3200: 034_005_003.mp4\nProcessing 2080/3200: 040_005_001.mp4\nProcessing 2090/3200: 004_004_003.mp4\nProcessing 2100/3200: 057_004_001.mp4\nProcessing 2110/3200: 060_010_003.mp4\nProcessing 2120/3200: 014_010_001.mp4\nProcessing 2130/3200: 001_001_004.mp4\nProcessing 2140/3200: 009_007_003.mp4\nProcessing 2150/3200: 021_001_002.mp4\nProcessing 2160/3200: 061_003_004.mp4\nProcessing 2170/3200: 020_003_004.mp4\nProcessing 2180/3200: 059_009_002.mp4\nProcessing 2190/3200: 051_004_003.mp4\nProcessing 2200/3200: 032_006_001.mp4\nProcessing 2210/3200: 059_009_004.mp4\nProcessing 2220/3200: 049_002_001.mp4\nProcessing 2230/3200: 023_008_005.mp4\nProcessing 2240/3200: 009_001_002.mp4\nProcessing 2250/3200: 063_002_004.mp4\nProcessing 2260/3200: 038_010_003.mp4\nProcessing 2270/3200: 053_006_004.mp4\nProcessing 2280/3200: 059_002_002.mp4\nProcessing 2290/3200: 020_008_003.mp4\nProcessing 2300/3200: 011_007_005.mp4\nProcessing 2310/3200: 032_009_001.mp4\nProcessing 2320/3200: 053_008_001.mp4\nProcessing 2330/3200: 006_009_002.mp4\nProcessing 2340/3200: 054_005_001.mp4\nProcessing 2350/3200: 060_008_005.mp4\nProcessing 2360/3200: 045_004_001.mp4\nProcessing 2370/3200: 064_008_004.mp4\nProcessing 2380/3200: 034_002_002.mp4\nProcessing 2390/3200: 049_009_001.mp4\nProcessing 2400/3200: 060_003_002.mp4\nProcessing 2410/3200: 035_003_004.mp4\nProcessing 2420/3200: 026_006_001.mp4\nProcessing 2430/3200: 048_007_005.mp4\nProcessing 2440/3200: 020_010_005.mp4\nProcessing 2450/3200: 042_003_001.mp4\nProcessing 2460/3200: 013_006_004.mp4\nProcessing 2470/3200: 056_003_001.mp4\nProcessing 2480/3200: 039_010_002.mp4\nProcessing 2490/3200: 003_004_004.mp4\nProcessing 2500/3200: 033_005_004.mp4\nProcessing 2510/3200: 043_007_005.mp4\nProcessing 2520/3200: 038_006_004.mp4\nProcessing 2530/3200: 021_005_004.mp4\nProcessing 2540/3200: 006_009_001.mp4\nProcessing 2550/3200: 063_006_004.mp4\nProcessing 2560/3200: 038_010_004.mp4\nProcessing 2570/3200: 015_005_001.mp4\nProcessing 2580/3200: 021_006_002.mp4\nProcessing 2590/3200: 015_006_005.mp4\nProcessing 2600/3200: 042_002_002.mp4\nProcessing 2610/3200: 026_001_003.mp4\nProcessing 2620/3200: 024_010_003.mp4\nProcessing 2630/3200: 052_002_001.mp4\nProcessing 2640/3200: 024_001_003.mp4\nProcessing 2650/3200: 055_009_005.mp4\nProcessing 2660/3200: 034_002_001.mp4\nProcessing 2670/3200: 035_008_004.mp4\nProcessing 2680/3200: 004_003_001.mp4\nProcessing 2690/3200: 013_009_003.mp4\nProcessing 2700/3200: 017_010_005.mp4\nProcessing 2710/3200: 005_003_001.mp4\nProcessing 2720/3200: 040_009_004.mp4\nProcessing 2730/3200: 049_004_004.mp4\nProcessing 2740/3200: 023_006_001.mp4\nProcessing 2750/3200: 018_006_001.mp4\nProcessing 2760/3200: 061_010_002.mp4\nProcessing 2770/3200: 053_002_001.mp4\nProcessing 2780/3200: 003_006_003.mp4\nProcessing 2790/3200: 009_010_001.mp4\nProcessing 2800/3200: 062_007_002.mp4\nProcessing 2810/3200: 023_005_005.mp4\nProcessing 2820/3200: 013_001_002.mp4\nProcessing 2830/3200: 039_008_003.mp4\nProcessing 2840/3200: 023_005_004.mp4\nProcessing 2850/3200: 002_009_005.mp4\nProcessing 2860/3200: 062_001_002.mp4\nProcessing 2870/3200: 062_002_003.mp4\nProcessing 2880/3200: 062_002_004.mp4\nProcessing 2890/3200: 044_008_003.mp4\nProcessing 2900/3200: 054_006_001.mp4\nProcessing 2910/3200: 003_010_003.mp4\nProcessing 2920/3200: 054_006_003.mp4\nProcessing 2930/3200: 055_003_003.mp4\nProcessing 2940/3200: 002_001_004.mp4\nProcessing 2950/3200: 033_004_005.mp4\nProcessing 2960/3200: 020_002_003.mp4\nProcessing 2970/3200: 034_006_003.mp4\nProcessing 2980/3200: 012_003_002.mp4\nProcessing 2990/3200: 052_003_002.mp4\nProcessing 3000/3200: 051_002_005.mp4\nProcessing 3010/3200: 019_010_001.mp4\nProcessing 3020/3200: 044_009_002.mp4\nProcessing 3030/3200: 034_007_002.mp4\nProcessing 3040/3200: 052_008_002.mp4\nProcessing 3050/3200: 040_002_002.mp4\nProcessing 3060/3200: 062_009_001.mp4\nProcessing 3070/3200: 019_010_002.mp4\nProcessing 3080/3200: 009_008_004.mp4\nProcessing 3090/3200: 010_004_002.mp4\nProcessing 3100/3200: 052_002_003.mp4\nProcessing 3110/3200: 007_001_002.mp4\nProcessing 3120/3200: 029_010_005.mp4\nProcessing 3130/3200: 023_004_003.mp4\nProcessing 3140/3200: 064_005_001.mp4\nProcessing 3150/3200: 005_009_003.mp4\nProcessing 3160/3200: 058_002_005.mp4\nProcessing 3170/3200: 009_009_005.mp4\nProcessing 3180/3200: 064_009_005.mp4\nProcessing 3190/3200: 032_004_001.mp4\n\nProcesamiento finalizado.\nTotal Videos: 3200\nData Secuencial Shape (DL): (3200, 40, 96)\nData Plana Shape (ML): (3200, 3840)\nClases detectadas: 64\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"### 4. Classification & Metrics\n\n# Split Data\nX_train_flat, X_test_flat, y_train_flat, y_test_flat = train_test_split(X_flat, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\nX_train_seq, X_test_seq, y_train_seq, y_test_seq = train_test_split(X_seq, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded)\n\n# Data Scaling (Standardization)\n# 1. Flat Data (ML)\nscaler_flat = StandardScaler()\nX_train_flat = scaler_flat.fit_transform(X_train_flat)\nX_test_flat = scaler_flat.transform(X_test_flat)\n\n# 2. Sequential Data (DL)\n# Scaler works on 2D, so we reshape (N, T, F) -> (N*T, F) then back\nN_train, T, F = X_train_seq.shape\nN_test = X_test_seq.shape[0]\n\nscaler_seq = StandardScaler()\nX_train_seq_2d = X_train_seq.reshape(-1, F)\nX_test_seq_2d = X_test_seq.reshape(-1, F)\n\nX_train_seq_2d = scaler_seq.fit_transform(X_train_seq_2d)\nX_test_seq_2d = scaler_seq.transform(X_test_seq_2d)\n\nX_train_seq = X_train_seq_2d.reshape(N_train, T, F)\nX_test_seq = X_test_seq_2d.reshape(N_test, T, F)\n\nresults_list = []\n\ndef get_model_size_mb(model, model_name, is_keras=False):\n    try:\n            if is_keras:\n                filepath = f\"/kaggle/working/entrenamiento_{model_name}.h5\"\n                model.save(filepath)\n            else:\n                filepath = f\"/kaggle/working/entrenamiento_{model_name}.joblib\"\n                joblib.dump(model, filepath)\n            size = os.path.getsize(filepath)\n            return size / (1024 * 1024) # MB\n    except Exception as e:\n        print(f\"Error calculating size for {model_name}: {e}\")\n        return 0\n\ndef plot_confusion_matrix_custom(y_true, y_pred, model_name, labels):\n    cm = confusion_matrix(y_true, y_pred)\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(cm, annot=False, fmt='d', cmap='Blues')\n    plt.title(f'Confusion Matrix - {model_name}')\n    plt.ylabel('True Label')\n    plt.xlabel('Predicted Label')\n    plt.tight_layout()\n    plt.savefig(f'/kaggle/working/confusion_matrix_{model_name}.png', dpi=300)\n    plt.close()\n\ndef evaluate_model(model, X_train, y_train, X_test, y_test, model_name, is_keras=False):\n    print(f\"\\nEvaluating {model_name}...\")\n    \n    # 1. Training Resources\n    tracemalloc.start()\n    start_time_train = time.time()\n    \n    if is_keras:\n        # EarlyStopping para DL\n        callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n        model.fit(X_train, y_train, epochs=40, batch_size=16, \n                  validation_split=0.1, callbacks=[callback], verbose=1)\n    else:\n        model.fit(X_train, y_train)\n        \n    train_time = time.time() - start_time_train\n    current, peak = tracemalloc.get_traced_memory()\n    peak_ram_mb = peak / (1024 * 1024)\n    tracemalloc.stop()\n    \n    # 2. Model Size\n    model_size_mb = get_model_size_mb(model, model_name, is_keras)\n    \n    # 3. Inference Time\n    start_time_inf = time.time()\n    if is_keras:\n        y_pred_probs = model.predict(X_test, verbose=0)\n        y_pred = np.argmax(y_pred_probs, axis=1)\n    else:\n        y_pred = model.predict(X_test)\n    inf_time_total = time.time() - start_time_inf\n    inf_time_per_sample_ms = (inf_time_total / len(X_test)) * 1000\n    \n    # 4. Metrics\n    acc = accuracy_score(y_test, y_pred)\n    prec, rec, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted', zero_division=0)\n    \n    # 5. Confusion Matrix\n    plot_confusion_matrix_custom(y_test, y_pred, model_name, class_names)\n    \n    print(f\"  Accuracy: {acc:.4f}\")\n    print(f\"  Training Time: {train_time:.2f}s\")\n    \n    return {\n        \"Model\": model_name,\n        \"Accuracy\": acc,\n        \"Precision\": prec,\n        \"Recall\": rec,\n        \"F1 Score\": f1,\n        \"Training Time (s)\": train_time,\n        \"Inference Time per Sample (ms)\": inf_time_per_sample_ms,\n        \"Model Size (MB)\": model_size_mb,\n        \"Peak RAM (MB)\": peak_ram_mb\n    }\n\n# --- A. Traditional ML ---\nml_classifiers = {\n    \"ExtraTrees\": ExtraTreesClassifier(n_estimators=100, random_state=42),\n    \"RandomForest\": RandomForestClassifier(n_estimators=100, random_state=42),\n    \"SVM\": SVC(kernel='rbf', random_state=42),\n    \"XGBoost\": xgb.XGBClassifier(eval_metric='mlogloss', random_state=42)\n}\n\nfor name, clf in ml_classifiers.items():\n    res = evaluate_model(clf, X_train_flat, y_train_flat, X_test_flat, y_test_flat, name, is_keras=False)\n    results_list.append(res)\n\n# --- B. Deep Learning ---\ndef build_lstm(input_shape, num_classes):\n    model = keras.Sequential([\n        layers.Input(shape=input_shape),\n        # Capa 1: Extracción de secuencias complejas\n        layers.LSTM(128, return_sequences=True),\n        layers.Dropout(0.3),\n        # Capa 2: Compresión y clasificación\n        layers.LSTM(64),\n        layers.Dropout(0.3),\n        layers.Dense(num_classes, activation='softmax')\n    ])\n    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    return model\n\ndef build_gru(input_shape, num_classes):\n    model = keras.Sequential([\n        layers.Input(shape=input_shape),\n        layers.GRU(128, return_sequences=True),\n        layers.Dropout(0.3),\n        layers.GRU(64),\n        layers.Dropout(0.3),\n        layers.Dense(num_classes, activation='softmax')\n    ])\n    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    return model\n\nclass PositionalEmbedding(layers.Layer):\n    def __init__(self, sequence_length, output_dim, **kwargs):\n        super().__init__(**kwargs)\n        self.position_embeddings = layers.Embedding(\n            input_dim=sequence_length, output_dim=output_dim\n        )\n        self.sequence_length = sequence_length\n        self.output_dim = output_dim\n        self.projection = layers.Dense(output_dim)\n\n    def call(self, inputs):\n        x = self.projection(inputs)\n        length = tf.shape(inputs)[1]\n        positions = tf.range(start=0, limit=length, delta=1)\n        embedded_positions = self.position_embeddings(positions)\n        return x + embedded_positions\n\n    def get_config(self):\n        config = super().get_config()\n        config.update({\n            \"sequence_length\": self.sequence_length,\n            \"output_dim\": self.output_dim,\n        })\n        return config\n\ndef build_transformer(input_shape, num_classes):\n    inputs = keras.Input(shape=input_shape)\n    \n    # --- Positional Embedding ---\n    d_model = 128\n    x = PositionalEmbedding(sequence_length=input_shape[0], output_dim=d_model)(inputs)\n    \n    # --- Transformer Block ---\n    # 1. Multi-Head Attention\n    x_norm = layers.LayerNormalization(epsilon=1e-6)(x)\n    attention_output = layers.MultiHeadAttention(key_dim=d_model, num_heads=4, dropout=0.1)(x_norm, x_norm)\n    x = layers.Add()([x, attention_output]) # Residual connection\n    \n    # 2. Feed Forward Network (FFN)\n    y = layers.LayerNormalization(epsilon=1e-6)(x)\n    y = layers.Conv1D(filters=d_model*2, kernel_size=1, activation='relu')(y) \n    y = layers.Dropout(0.1)(y)\n    y = layers.Conv1D(filters=d_model, kernel_size=1)(y) \n    x = layers.Add()([x, y]) # Residual connection\n    \n    # --- Classification Head ---\n    x = layers.GlobalAveragePooling1D()(x)\n    x = layers.Dropout(0.3)(x)\n    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n    \n    model = keras.Model(inputs, outputs)\n    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    return model\n\ndl_constructors = {\n    \"LSTM\": build_lstm,\n    \"GRU\": build_gru,\n    \"Transformer\": build_transformer\n}\n\nfor name, constructor in dl_constructors.items():\n    model = constructor((40, 96), num_classes)\n    res = evaluate_model(model, X_train_seq, y_train_seq, X_test_seq, y_test_seq, name, is_keras=True)\n    results_list.append(res)\n\n\n# --- C. Export & Analysis ---\ndf_results = pd.DataFrame(results_list)\ndf_results.to_csv(\"/kaggle/working/metrics_results.csv\", index=False)\nprint(\"\\nMetrics exported to metrics_results.csv\")\nprint(df_results)\n\n# --- D. Comparative Plots ---\n\n# 1. Training & Inference Time\ndf_melt_time = df_results.melt(id_vars=\"Model\", value_vars=[\"Training Time (s)\", \"Inference Time per Sample (ms)\"], var_name=\"Metric\", value_name=\"Time\")\nplt.figure(figsize=(12, 6))\nsns.barplot(data=df_melt_time, x=\"Model\", y=\"Time\", hue=\"Metric\")\nplt.title(\"Training vs Inference Time\")\nplt.yscale(\"log\") # Log scale usually better for these comparisons\nplt.ylabel(\"Time (Log Scale)\")\nplt.tight_layout()\nplt.savefig(\"/kaggle/working/plot_time_comparison.png\", dpi=300)\nplt.close()\n\n# 2. Disk Size & Peak RAM\ndf_melt_size = df_results.melt(id_vars=\"Model\", value_vars=[\"Model Size (MB)\", \"Peak RAM (MB)\"], var_name=\"Metric\", value_name=\"Size (MB)\")\nplt.figure(figsize=(12, 6))\nsns.barplot(data=df_melt_size, x=\"Model\", y=\"Size (MB)\", hue=\"Metric\")\nplt.title(\"Model Size vs Peak RAM\")\nplt.tight_layout()\nplt.savefig(\"/kaggle/working/plot_size_ram_comparison.png\", dpi=300)\nplt.close()\n\n# 3. Accuracy vs Inference Time (Scatter)\nplt.figure(figsize=(12, 8))\nsns.scatterplot(data=df_results, x=\"Inference Time per Sample (ms)\", y=\"Accuracy\", s=300, hue=\"Model\", style=\"Model\")\nfor i in range(df_results.shape[0]):\n    plt.text(df_results[\"Inference Time per Sample (ms)\"][i], df_results[\"Accuracy\"][i]+0.005, \n             df_results[\"Model\"][i], fontsize=10, ha='center')\nplt.title(\"Accuracy vs Inference Time\")\nplt.grid(True)\nplt.tight_layout()\nplt.savefig(\"/kaggle/working/plot_accuracy_vs_inference.png\", dpi=300)\nplt.close()\n\nprint(\"\\nAll plots saved as PNG images.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-05T06:23:08.395234Z","iopub.execute_input":"2026-01-05T06:23:08.395521Z","iopub.status.idle":"2026-01-05T06:31:11.667496Z","shell.execute_reply.started":"2026-01-05T06:23:08.395502Z","shell.execute_reply":"2026-01-05T06:31:11.666768Z"}},"outputs":[{"name":"stdout","text":"\nEvaluating ExtraTrees...\n  Accuracy: 0.9703\n  Training Time: 3.32s\n\nEvaluating RandomForest...\n  Accuracy: 0.9625\n  Training Time: 21.96s\n\nEvaluating SVM...\n  Accuracy: 0.9219\n  Training Time: 7.76s\n\nEvaluating XGBoost...\n  Accuracy: 0.9359\n  Training Time: 287.41s\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1767594522.124991      47 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1767594522.125686      47 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"},{"name":"stdout","text":"\nEvaluating LSTM...\nEpoch 1/40\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1767594533.767029   12964 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 19ms/step - accuracy: 0.1944 - loss: 3.5824 - val_accuracy: 0.4883 - val_loss: 2.3660\nEpoch 2/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.5448 - loss: 2.0094 - val_accuracy: 0.7031 - val_loss: 1.3505\nEpoch 3/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6970 - loss: 1.2481 - val_accuracy: 0.7969 - val_loss: 0.8809\nEpoch 4/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7859 - loss: 0.8663 - val_accuracy: 0.8281 - val_loss: 0.7053\nEpoch 5/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8300 - loss: 0.6614 - val_accuracy: 0.8438 - val_loss: 0.6258\nEpoch 6/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8517 - loss: 0.5843 - val_accuracy: 0.8828 - val_loss: 0.4656\nEpoch 7/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.8776 - loss: 0.4586 - val_accuracy: 0.8945 - val_loss: 0.3599\nEpoch 8/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9153 - loss: 0.3493 - val_accuracy: 0.9258 - val_loss: 0.2832\nEpoch 9/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9191 - loss: 0.3109 - val_accuracy: 0.8945 - val_loss: 0.2935\nEpoch 10/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9191 - loss: 0.2776 - val_accuracy: 0.9102 - val_loss: 0.2818\nEpoch 11/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9285 - loss: 0.2610 - val_accuracy: 0.9219 - val_loss: 0.3012\nEpoch 12/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9341 - loss: 0.2290 - val_accuracy: 0.9414 - val_loss: 0.2089\nEpoch 13/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9488 - loss: 0.1963 - val_accuracy: 0.9414 - val_loss: 0.2401\nEpoch 14/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9439 - loss: 0.2211 - val_accuracy: 0.9453 - val_loss: 0.1833\nEpoch 15/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9631 - loss: 0.1572 - val_accuracy: 0.9492 - val_loss: 0.1879\nEpoch 16/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9590 - loss: 0.1369 - val_accuracy: 0.9219 - val_loss: 0.2120\nEpoch 17/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.9685 - loss: 0.1339 - val_accuracy: 0.9336 - val_loss: 0.2023\nEpoch 18/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9675 - loss: 0.1274 - val_accuracy: 0.9727 - val_loss: 0.1300\nEpoch 19/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9849 - loss: 0.0760 - val_accuracy: 0.9688 - val_loss: 0.1285\nEpoch 20/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9859 - loss: 0.0671 - val_accuracy: 0.9727 - val_loss: 0.1124\nEpoch 21/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9844 - loss: 0.0654 - val_accuracy: 0.9258 - val_loss: 0.1918\nEpoch 22/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9569 - loss: 0.1649 - val_accuracy: 0.9219 - val_loss: 0.2232\nEpoch 23/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9603 - loss: 0.1555 - val_accuracy: 0.9648 - val_loss: 0.1739\nEpoch 24/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9752 - loss: 0.0981 - val_accuracy: 0.9727 - val_loss: 0.1269\nEpoch 25/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9722 - loss: 0.1053 - val_accuracy: 0.9570 - val_loss: 0.1503\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"  Accuracy: 0.9609\n  Training Time: 47.79s\n\nEvaluating GRU...\nEpoch 1/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 18ms/step - accuracy: 0.1450 - loss: 3.5934 - val_accuracy: 0.4961 - val_loss: 2.4015\nEpoch 2/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.4571 - loss: 2.2184 - val_accuracy: 0.6914 - val_loss: 1.5856\nEpoch 3/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.6387 - loss: 1.4662 - val_accuracy: 0.7812 - val_loss: 1.0228\nEpoch 4/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.7830 - loss: 0.9707 - val_accuracy: 0.8516 - val_loss: 0.7001\nEpoch 5/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.8676 - loss: 0.6580 - val_accuracy: 0.8711 - val_loss: 0.5015\nEpoch 6/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9120 - loss: 0.4748 - val_accuracy: 0.8984 - val_loss: 0.3556\nEpoch 7/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9243 - loss: 0.3561 - val_accuracy: 0.9297 - val_loss: 0.2662\nEpoch 8/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9545 - loss: 0.2492 - val_accuracy: 0.9414 - val_loss: 0.2356\nEpoch 9/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9601 - loss: 0.2148 - val_accuracy: 0.9492 - val_loss: 0.1855\nEpoch 10/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9716 - loss: 0.1633 - val_accuracy: 0.9414 - val_loss: 0.1881\nEpoch 11/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9795 - loss: 0.1377 - val_accuracy: 0.9648 - val_loss: 0.1666\nEpoch 12/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9689 - loss: 0.1327 - val_accuracy: 0.9766 - val_loss: 0.1129\nEpoch 13/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9686 - loss: 0.1332 - val_accuracy: 0.9648 - val_loss: 0.1415\nEpoch 14/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9807 - loss: 0.0907 - val_accuracy: 0.9609 - val_loss: 0.1479\nEpoch 15/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9853 - loss: 0.0807 - val_accuracy: 0.9766 - val_loss: 0.1094\nEpoch 16/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9903 - loss: 0.0593 - val_accuracy: 0.9727 - val_loss: 0.1062\nEpoch 17/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9865 - loss: 0.0667 - val_accuracy: 0.9805 - val_loss: 0.1032\nEpoch 18/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9946 - loss: 0.0430 - val_accuracy: 0.9609 - val_loss: 0.1206\nEpoch 19/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9915 - loss: 0.0418 - val_accuracy: 0.9531 - val_loss: 0.1602\nEpoch 20/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9733 - loss: 0.1131 - val_accuracy: 0.9375 - val_loss: 0.2337\nEpoch 21/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9539 - loss: 0.1646 - val_accuracy: 0.9609 - val_loss: 0.1181\nEpoch 22/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9885 - loss: 0.0537 - val_accuracy: 0.9844 - val_loss: 0.0865\nEpoch 23/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9958 - loss: 0.0306 - val_accuracy: 0.9766 - val_loss: 0.0909\nEpoch 24/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9901 - loss: 0.0431 - val_accuracy: 0.9805 - val_loss: 0.0854\nEpoch 25/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9923 - loss: 0.0307 - val_accuracy: 0.9805 - val_loss: 0.0994\nEpoch 26/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9902 - loss: 0.0407 - val_accuracy: 0.9844 - val_loss: 0.0766\nEpoch 27/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9932 - loss: 0.0320 - val_accuracy: 0.9844 - val_loss: 0.0805\nEpoch 28/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9950 - loss: 0.0274 - val_accuracy: 0.9805 - val_loss: 0.0637\nEpoch 29/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9949 - loss: 0.0265 - val_accuracy: 0.9648 - val_loss: 0.2187\nEpoch 30/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9857 - loss: 0.0512 - val_accuracy: 0.9766 - val_loss: 0.0899\nEpoch 31/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9942 - loss: 0.0226 - val_accuracy: 0.9844 - val_loss: 0.0805\nEpoch 32/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9830 - loss: 0.0532 - val_accuracy: 0.9219 - val_loss: 0.3499\nEpoch 33/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9670 - loss: 0.1050 - val_accuracy: 0.9453 - val_loss: 0.2148\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"  Accuracy: 0.9859\n  Training Time: 55.69s\n\nEvaluating Transformer...\nEpoch 1/40\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1767594646.870629   12964 service.cc:148] XLA service 0x160494c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1767594646.871989   12964 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1767594646.872011   12964 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m 27/144\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.0829 - loss: 4.2601   ","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1767594653.279212   12964 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 21ms/step - accuracy: 0.2314 - loss: 3.2202 - val_accuracy: 0.6758 - val_loss: 1.1241\nEpoch 2/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6687 - loss: 1.1109 - val_accuracy: 0.8281 - val_loss: 0.5721\nEpoch 3/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8047 - loss: 0.6227 - val_accuracy: 0.8867 - val_loss: 0.3535\nEpoch 4/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8655 - loss: 0.4301 - val_accuracy: 0.9062 - val_loss: 0.2829\nEpoch 5/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8999 - loss: 0.2897 - val_accuracy: 0.8750 - val_loss: 0.4224\nEpoch 6/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8893 - loss: 0.3251 - val_accuracy: 0.9609 - val_loss: 0.1414\nEpoch 7/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9278 - loss: 0.1961 - val_accuracy: 0.9531 - val_loss: 0.1988\nEpoch 8/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9216 - loss: 0.2283 - val_accuracy: 0.9375 - val_loss: 0.2705\nEpoch 9/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9355 - loss: 0.1728 - val_accuracy: 0.9414 - val_loss: 0.1709\nEpoch 10/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9656 - loss: 0.1184 - val_accuracy: 0.9219 - val_loss: 0.2773\nEpoch 11/40\n\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9389 - loss: 0.1969 - val_accuracy: 0.9648 - val_loss: 0.1722\n","output_type":"stream"},{"name":"stderr","text":"WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n","output_type":"stream"},{"name":"stdout","text":"  Accuracy: 0.9469\n  Training Time: 35.00s\n\nMetrics exported to metrics_results.csv\n          Model  Accuracy  Precision    Recall  F1 Score  Training Time (s)  \\\n0    ExtraTrees  0.970313   0.972990  0.970313  0.970272           3.317426   \n1  RandomForest  0.962500   0.968386  0.962500  0.962184          21.963737   \n2           SVM  0.921875   0.933647  0.921875  0.920125           7.756842   \n3       XGBoost  0.935937   0.940250  0.935937  0.935365         287.410970   \n4          LSTM  0.960938   0.968711  0.960938  0.959891          47.790968   \n5           GRU  0.985938   0.987453  0.985938  0.985680          55.687465   \n6   Transformer  0.946875   0.952556  0.946875  0.945312          35.003115   \n\n   Inference Time per Sample (ms)  Model Size (MB)  Peak RAM (MB)  \n0                        0.074816        56.132226      41.321878  \n1                        0.069121        27.803079      41.317204  \n2                        8.663294        66.292380      82.655665  \n3                        0.033410         4.595612       4.683777  \n4                        0.733329         1.968887      71.890910  \n5                        0.600627         1.504501      71.450990  \n6                        2.052351         4.147141      71.450052  \n\nAll plots saved as PNG images.\n","output_type":"stream"}],"execution_count":5}]}
